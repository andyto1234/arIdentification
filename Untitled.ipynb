{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "heard-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "electrical-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 256, 256\n",
    "\n",
    "train_data_dir = '/Users/ato/scripts/python_code/arIdentification/data/training'\n",
    "validation_data_dir = '/Users/ato/scripts/python_code/arIdentification/data/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "recreational-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "integrated-hungarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 538 images belonging to 2 classes.\n",
      "Found 101 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=64,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "intended-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(img_width, img_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "certain-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "destroyed-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bronze-general",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7810 - accuracy: 0.4009WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 101 batches). You may need to use the repeat() function when building your dataset.\n",
      "13/13 [==============================] - 12s 799ms/step - loss: 0.7793 - accuracy: 0.4029 - val_loss: 0.7855 - val_accuracy: 0.1188\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 9s 714ms/step - loss: 0.7305 - accuracy: 0.4702\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 9s 712ms/step - loss: 0.7142 - accuracy: 0.4817\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 10s 730ms/step - loss: 0.7070 - accuracy: 0.4990\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 10s 719ms/step - loss: 0.7215 - accuracy: 0.4287\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 10s 735ms/step - loss: 0.7026 - accuracy: 0.4925\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 10s 739ms/step - loss: 0.6963 - accuracy: 0.5195\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 10s 733ms/step - loss: 0.7119 - accuracy: 0.4674\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 10s 753ms/step - loss: 0.7010 - accuracy: 0.4854\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 10s 730ms/step - loss: 0.6905 - accuracy: 0.5311\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 10s 736ms/step - loss: 0.6825 - accuracy: 0.5322\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 10s 782ms/step - loss: 0.6898 - accuracy: 0.5380\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 10s 768ms/step - loss: 0.6833 - accuracy: 0.5669\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 10s 735ms/step - loss: 0.6923 - accuracy: 0.5156\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 10s 740ms/step - loss: 0.6922 - accuracy: 0.5035\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 10s 743ms/step - loss: 0.6948 - accuracy: 0.5645\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 10s 740ms/step - loss: 0.6990 - accuracy: 0.5044\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 10s 732ms/step - loss: 0.6929 - accuracy: 0.5309\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 10s 734ms/step - loss: 0.6981 - accuracy: 0.5512\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 10s 736ms/step - loss: 0.6983 - accuracy: 0.4998\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 10s 741ms/step - loss: 0.6939 - accuracy: 0.5673\n",
      "Epoch 22/30\n",
      "13/13 [==============================] - 10s 744ms/step - loss: 0.6800 - accuracy: 0.5632\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 10s 734ms/step - loss: 0.6756 - accuracy: 0.5860\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 10s 740ms/step - loss: 0.6800 - accuracy: 0.6174\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 10s 738ms/step - loss: 0.6786 - accuracy: 0.5862\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 10s 741ms/step - loss: 0.6794 - accuracy: 0.5799\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 10s 747ms/step - loss: 0.6913 - accuracy: 0.5414\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 10s 746ms/step - loss: 0.6759 - accuracy: 0.5993\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 10s 747ms/step - loss: 0.6834 - accuracy: 0.5896\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 10s 742ms/step - loss: 0.6936 - accuracy: 0.5719\n"
     ]
    }
   ],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(12))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "nb_epoch = 30\n",
    "nb_train_samples = 538\n",
    "nb_validation_samples = 101\n",
    "\n",
    "model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=426//32,\n",
    "        epochs=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples)\n",
    "\n",
    "model.save_weights('/Users/ato/scripts/python_code/arIdentification/models/basic_cnn_20_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "golden-marketplace",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'image' from 'PIL' (/Users/ato/anaconda3/lib/python3.7/site-packages/PIL/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-833de98b3a9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/ato/scripts/python_code/arIdentification/test/20210302_000700_n4euA_171.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'image' from 'PIL' (/Users/ato/anaconda3/lib/python3.7/site-packages/PIL/__init__.py)"
     ]
    }
   ],
   "source": [
    "from PIL import image\n",
    "\n",
    "img = image.load_img('/Users/ato/scripts/python_code/arIdentification/test/20210302_000700_n4euA_171.jpg', target_size=(256,256))\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "nervous-hartford",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'PIL.JpegImagePlugin.JpegImageFile'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-191b964758ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1097\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 964\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    965\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'PIL.JpegImagePlugin.JpegImageFile'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "prediction =model.predict(img)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-recommendation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
